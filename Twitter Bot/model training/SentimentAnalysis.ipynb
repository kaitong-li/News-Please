{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "TENSORFLOW_GPU",
      "language": "python",
      "name": "tensorflow-gpu"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Linear_svm_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuIzzzmNSXn1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4321f423-49f0-42ca-ea61-3e5bf3bcc980"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split,KFold,StratifiedKFold\n",
        "from sklearn import model_selection, preprocessing, metrics, svm, ensemble\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
        "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.externals import joblib\n",
        "import pickle\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import json, nltk\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vVm_eVxSkJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13ee36b2-ea3e-4002-a732-c7079b41c9a5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RipijHu6SXn6"
      },
      "source": [
        "total_data = pd.read_csv('/content/drive/MyDrive/Twitter Bot/data/train1.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/Twitter Bot/data/test1.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwrFPtdDplsY"
      },
      "source": [
        "with open('/content/drive/MyDrive/Twitter Bot/data/contractions.json', 'r') as f:\n",
        "    contractions_dict = json.load(f)\n",
        "contractions = contractions_dict['contractions']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsVon8FrpsVJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "458fbe1e-3c42-4ab6-9684-6d30229c15d7"
      },
      "source": [
        "pd.set_option('display.max_colwidth', -1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtwpOZRwsx3P"
      },
      "source": [
        "label = total_data.columns.values[0]\n",
        "tweet = total_data.columns.values[1]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgHUyvIxoUcp"
      },
      "source": [
        "import re\n",
        "\n",
        "def emoji(tweet):\n",
        "    # Smile -- :), : ), :-), (:, ( :, (-:, :') , :O\n",
        "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\)|:O)', ' positiveemoji ', tweet)\n",
        "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
        "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' positiveemoji ', tweet)\n",
        "    # Love -- <3, :*\n",
        "    tweet = re.sub(r'(<3|:\\*)', ' positiveemoji ', tweet)\n",
        "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-; , @-)\n",
        "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;|@-\\))', ' positiveemoji ', tweet)\n",
        "    # Sad -- :-(, : (, :(, ):, )-:, :-/ , :-|\n",
        "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:|:-/|:-\\|)', ' negetiveemoji ', tweet)\n",
        "    # Cry -- :,(, :'(, :\"(\n",
        "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' negetiveemoji ', tweet)\n",
        "    return tweet\n",
        "def process_tweet(tweet):\n",
        "    tweet = tweet.lower()                                             # Lowercases the string\n",
        "    tweet = re.sub('@[^\\s]+', '', tweet)                              # Removes usernames\n",
        "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', ' ', tweet)   # Remove URLs\n",
        "    tweet = re.sub(r\"\\d+\", \" \", str(tweet))                           # Removes all digits\n",
        "    tweet = re.sub('&quot;',\" \", tweet)                               # Remove (&quot;) \n",
        "    tweet = emoji(tweet)                                              # Replaces Emojis\n",
        "    tweet = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", str(tweet))                   # Removes all single characters\n",
        "    for word in tweet.split():\n",
        "        if word.lower() in contractions:\n",
        "            tweet = tweet.replace(word, contractions[word.lower()])   # Replaces contractions\n",
        "    tweet = re.sub(r\"[^\\w\\s]\", \" \", str(tweet))                       # Removes all punctuations\n",
        "    tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)                         # Convert more than 2 letter repetitions to 2 letter\n",
        "    tweet = re.sub(r\"\\s+\", \" \", str(tweet))                           # Replaces double spaces with single space    \n",
        "    return tweet"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCqp4Ro-oW_h"
      },
      "source": [
        "tweet = total_data.columns.values[1]\n",
        "total_data['cleaned_tweets'] = np.vectorize(process_tweet)(total_data[tweet])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1NH-qIXyAkU"
      },
      "source": [
        "test_tweet = test.columns.values[0]\n",
        "test['cleaned_tweets'] = np.vectorize(process_tweet)(test[test_tweet])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHEy17hNSXn-",
        "outputId": "39e44701-1a5d-4599-a170-c395f64e0187"
      },
      "source": [
        "params = {'tfidf__max_df': [0.9, 0.95],'tfidf__ngram_range': [(1,1), (1,2)], \"svc__C\": [0.001,.01, .1, 1, 10, 100]}\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(sublinear_tf=True, stop_words='english')),\n",
        "    (\"svc\", LinearSVC(penalty='l2',dual=False,random_state=0, max_iter=1000,tol=0.01)),\n",
        "])\n",
        "gs = GridSearchCV(pipeline, params, cv=10, verbose=2, n_jobs=-1)\n",
        "gs.fit(total_data['cleaned_tweets'], total_data['label'])\n",
        "joblib.dump(gs, '/content/drive/MyDrive/Twitter Bot/data/sentimentAnalysis.pkl')\n",
        "print(gs.best_estimator_)\n",
        "print(gs.best_score_)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  6.4min\n",
            "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 10.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Pipeline(memory=None,\n",
            "         steps=[('tfidf',\n",
            "                 TfidfVectorizer(analyzer='word', binary=False,\n",
            "                                 decode_error='strict',\n",
            "                                 dtype=<class 'numpy.float64'>,\n",
            "                                 encoding='utf-8', input='content',\n",
            "                                 lowercase=True, max_df=0.9, max_features=None,\n",
            "                                 min_df=1, ngram_range=(1, 2), norm='l2',\n",
            "                                 preprocessor=None, smooth_idf=True,\n",
            "                                 stop_words='english', strip_accents=None,\n",
            "                                 sublinear_tf=True,\n",
            "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                                 tokenizer=None, use_idf=True,\n",
            "                                 vocabulary=None)),\n",
            "                ('svc',\n",
            "                 LinearSVC(C=0.1, class_weight=None, dual=False,\n",
            "                           fit_intercept=True, intercept_scaling=1,\n",
            "                           loss='squared_hinge', max_iter=1000,\n",
            "                           multi_class='ovr', penalty='l2', random_state=0,\n",
            "                           tol=0.01, verbose=0))],\n",
            "         verbose=False)\n",
            "0.7473003154800889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufaXhEdxLMRN"
      },
      "source": [
        "svr_model = joblib.load('/content/drive/MyDrive/Twitter Bot/data/sentimentAnalysis.pkl')"
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}